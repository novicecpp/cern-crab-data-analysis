{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3978c4d",
   "metadata": {},
   "source": [
    "## Load raw data from HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a2df42",
   "metadata": {},
   "source": [
    "### 1. Define the default HDFS folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e830d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_DEFAULT_HDFS_FOLDER = \"/project/monitoring/archive/condor/raw/metric\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c77f47",
   "metadata": {},
   "source": [
    "### 2. Define the data structure needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a30563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_schema():\n",
    "    return StructType(\n",
    "        [\n",
    "            StructField(\n",
    "                \"data\",\n",
    "                StructType(\n",
    "                    [\n",
    "                        StructField(\"CMSSite\", StringType(), nullable=True),\n",
    "                        StructField(\"RecordTime\", LongType(), nullable=False),\n",
    "                        StructField(\"InputData\", StringType(), nullable=True),\n",
    "                        StructField(\"CMSPrimaryDataTier\", StringType(), nullable=True),\n",
    "                        StructField(\"Status\", StringType(), nullable=True),\n",
    "                        StructField(\"OverflowType\", StringType(), nullable=True),\n",
    "                        StructField(\"WallClockHr\", DoubleType(), nullable=True),\n",
    "                        StructField(\"CoreHr\", DoubleType(), nullable=True),\n",
    "                        StructField(\"CpuTimeHr\", DoubleType(), nullable=True),\n",
    "                        StructField(\"RequestCpus\", LongType(), nullable=True),\n",
    "                        StructField(\"Type\", StringType(), nullable=True),\n",
    "                        StructField(\"CRAB_DataBlock\", StringType(), nullable=True),\n",
    "                        StructField(\"GlobalJobId\", StringType(), nullable=False),\n",
    "                        StructField(\"ExitCode\", LongType(), nullable=True),\n",
    "                        StructField(\"Chirp_CRAB3_Job_ExitCode\", LongType(), nullable=True),\n",
    "                        StructField(\"Chirp_WMCore_cmsRun_ExitCode\", LongType(), nullable=True),\n",
    "                        StructField(\"JobExitCode\", LongType(), nullable=True)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b3d0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = _get_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adad287",
   "metadata": {},
   "source": [
    "### 3. Get the candidate file that contains the date needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1ada98",
   "metadata": {},
   "source": [
    "This function returns the candidate files by looking at a 7-days timespan (3 days before the specified date, current date, and 3 days after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ce670e",
   "metadata": {},
   "source": [
    "Reference: https://github.com/dmwm/CMSSpark/blob/cd1a4725601a3c3679f27b7439aa34d16f1442a2/src/python/CMSSpark/condor_cpu_efficiency.py#L138"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6415cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate_files(\n",
    "    start_date, end_date, spark, base=_DEFAULT_HDFS_FOLDER,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns a list of hdfs folders that can contain data for the given dates.\n",
    "    \"\"\"\n",
    "    st_date = start_date - timedelta(days=3)\n",
    "    ed_date = end_date + timedelta(days=3)\n",
    "    days = (ed_date - st_date).days\n",
    "    #what is pre_candidate_files for????\n",
    "    pre_candidate_files = [\n",
    "        \"{base}/{day}{{,.tmp}}\".format(\n",
    "            base=base, day=(st_date + timedelta(days=i)).strftime(\"%Y/%m/%d\")\n",
    "        )\n",
    "        for i in range(0, days)\n",
    "    ]\n",
    "    sc = spark.sparkContext\n",
    "    # The candidate files are the folders to the specific dates,\n",
    "    # but if we are looking at recent days the compaction procedure could\n",
    "    # have not run yet so we will considerate also the .tmp folders.\n",
    "    candidate_files = [\n",
    "        f\"{base}/{(st_date + timedelta(days=i)).strftime('%Y/%m/%d')}\"\n",
    "        for i in range(0, days)\n",
    "    ]\n",
    "    FileSystem = sc._gateway.jvm.org.apache.hadoop.fs.FileSystem\n",
    "    URI = sc._gateway.jvm.java.net.URI\n",
    "    Path = sc._gateway.jvm.org.apache.hadoop.fs.Path\n",
    "    fs = FileSystem.get(URI(\"hdfs:///\"), sc._jsc.hadoopConfiguration())\n",
    "    candidate_files = [url for url in candidate_files if fs.globStatus(Path(url))]\n",
    "    return candidate_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef785bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime(2022, 5, 1)\n",
    "end_date = datetime(2022, 5, 2)\n",
    "get_candidate_files(start_date, end_date, spark, base=_DEFAULT_HDFS_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eebade",
   "metadata": {},
   "source": [
    "### 4. Read raw data to spark dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568e14bd",
   "metadata": {},
   "source": [
    "Modify the filter to suit your purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62775c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = (\n",
    "        spark.read.option(\"basePath\", _DEFAULT_HDFS_FOLDER)\n",
    "        .json(\n",
    "            get_candidate_files(start_date, end_date, spark, base=_DEFAULT_HDFS_FOLDER),\n",
    "            schema=schema,\n",
    "        ).select(\"data.*\")\n",
    "        .filter(\n",
    "            f\"\"\"Status IN ('Completed', 'Removed', 'Held', 'Error') \n",
    "          AND RecordTime >= {start_date.timestamp() * 1000}\n",
    "          AND RecordTime < {end_date.timestamp() * 1000}\n",
    "          \"\"\"\n",
    "        )\n",
    "        .drop_duplicates([\"GlobalJobId\"])\n",
    "    )\n",
    "\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970b6236",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
